#!/usr/bin/env python3
"""
GRPO Evaluation Test Script

This script provides a simplified way to run GRPO evaluations for testing purposes.
It downloads a dataset from S3, prepares the environment, and runs the evaluation
in a Docker container similar to how it's done in production.

Usage:
    python run_grpo_evaluation.py <s3_dataset_url> <original_model> <model1,model2,...>

Example:
    python run_grpo_evaluation.py https://bucket.s3.amazonaws.com/dataset.jsonl \
        mistralai/Mistral-7B-v0.1 repo1/adapter,repo2/adapter
"""

import argparse
import os
import shutil
import subprocess
import sys
import tempfile
from pathlib import Path
from urllib.parse import urlparse

import aiohttp
import asyncio


async def download_file(url, local_path):
    """Download a file from a URL to a local path."""
    print(f"Downloading {url} to {local_path}...")
    
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                with open(local_path, "wb") as f:
                    f.write(await response.read())
                print(f"Download completed: {local_path}")
                return local_path
            else:
                raise Exception(f"Failed to download file: {response.status}")


def run_docker_evaluation(dataset_path, original_model, models, dataset_type='{"field_prompt":"prompt"}', file_format="jsonl"):
    """Run the GRPO evaluation in a Docker container."""
    dataset_dir = os.path.dirname(os.path.abspath(dataset_path))
    dataset_filename = os.path.basename(dataset_path)
    
    print(f"Running GRPO evaluation for models: {models}")
    
    # Map required environment variables
    env = {
        "DATASET": f"/workspace/input_data/{dataset_filename}",
        "MODELS": models,
        "ORIGINAL_MODEL": original_model,
        "DATASET_TYPE": dataset_type,
        "FILE_FORMAT": file_format,
    }
    
    # Build the docker command
    cmd = [
        "docker", "run", "--rm",
        "--runtime", "nvidia",
        "--gpus", "all",
    ]
    
    # Add environment variables
    for key, value in env.items():
        cmd.extend(["-e", f"{key}={value}"])
    
    # Add volume mounts
    cmd.extend([
        "-v", f"{dataset_dir}:/workspace/input_data:ro",
        "-v", f"{os.path.expanduser('~/.cache/huggingface')}:/root/.cache/huggingface:rw",
        "validator",
        "python", "-m", "validator.evaluation.eval_grpo"
    ])
    
    # Run the docker command
    print("Executing docker command:")
    print(" ".join(cmd))
    
    try:
        subprocess.run(cmd, check=True)
        print("Docker evaluation completed successfully")
    except subprocess.CalledProcessError as e:
        print(f"Docker evaluation failed with exit code {e.returncode}")
        raise


async def main():
    parser = argparse.ArgumentParser(description="Run GRPO evaluation for testing")
    parser.add_argument("dataset_url", help="S3 URL to the dataset")
    parser.add_argument("original_model", help="Original base model to use")
    parser.add_argument("models", help="Comma-separated list of model repos to evaluate")
    
    args = parser.parse_args()
    
    # Create temporary directory for dataset
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # Download dataset from S3
            dataset_filename = os.path.basename(urlparse(args.dataset_url).path)
            local_dataset_path = os.path.join(temp_dir, dataset_filename)
            
            await download_file(args.dataset_url, local_dataset_path)
            
            # Run docker evaluation
            run_docker_evaluation(
                dataset_path=local_dataset_path,
                original_model=args.original_model,
                models=args.models
            )
            
            print("GRPO evaluation completed")
            
        except Exception as e:
            print(f"Error: {e}")
            sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())